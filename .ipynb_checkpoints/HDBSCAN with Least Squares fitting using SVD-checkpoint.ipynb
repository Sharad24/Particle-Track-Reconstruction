{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e081740e-8169-4481-b1df-f5dd5488314f",
    "_uuid": "0bee86255243664f24e4bcf48af2228a3100a8b7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "572fcbb6-8c7b-4a09-8916-8ec76689130f",
    "_uuid": "63414de98667e95f60407c9155899a25a321cffc"
   },
   "outputs": [],
   "source": [
    "# Change this according to your directory preferred setting\n",
    "path_to_train = \"../train_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d0f5916b-8270-4ff5-af17-6fbbb8d00553",
    "_uuid": "3e45554ab05c14faf63a2c423f69ebbe7c108541"
   },
   "outputs": [],
   "source": [
    "# This event is in Train_1\n",
    "event_prefix = \"event000001000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "595f01a7-fa03-4398-abb5-2354ca359fa6",
    "_uuid": "0ace6a8761680565b177f0a1b12f85949fecb599"
   },
   "outputs": [],
   "source": [
    "hits, cells, particles, truth = load_event(os.path.join(path_to_train, event_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "e06d1ed7-5091-4d67-abb4-5984b137e2e6",
    "_uuid": "c2f70ae63abffcc09a534bb17fb89df8ffddb722",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-941a413f4344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "class Clusterer(object):\n",
    "    def __init__(self,rz_scales=[0.65, 0.965, 1.528]):                        \n",
    "        self.rz_scales=rz_scales\n",
    "    \n",
    "    def _eliminate_outliers(self,labels,M):\n",
    "        norms=np.zeros((len(labels)),np.float32)\n",
    "        indices=np.zeros((len(labels)),np.float32)\n",
    "        for i, cluster in tqdm(enumerate(labels),total=len(labels)):\n",
    "            if cluster == 0:\n",
    "                continue\n",
    "            index = np.argwhere(self.clusters==cluster)\n",
    "            index = np.reshape(index,(index.shape[0]))\n",
    "            indices[i] = len(index)\n",
    "            x = M[index]\n",
    "            norms[i] = self._test_quadric(x)\n",
    "        threshold1 = np.percentile(norms,90)*5\n",
    "        threshold2 = 25\n",
    "        threshold3 = 6\n",
    "        for i, cluster in enumerate(labels):\n",
    "            if norms[i] > threshold1 or indices[i] > threshold2 or indices[i] < threshold3:\n",
    "                self.clusters[self.clusters==cluster]=0   \n",
    "    def _test_quadric(self,x):\n",
    "        if x.size == 0 or len(x.shape)<2:\n",
    "            return 0\n",
    "        xm = np.mean(x,axis=0)\n",
    "        x = x - xm\n",
    "        Z = np.zeros((x.shape[0],10), np.float32)\n",
    "        Z[:,0] = x[:,0]**2\n",
    "        Z[:,1] = 2*x[:,0]*x[:,1]\n",
    "        Z[:,2] = 2*x[:,0]*x[:,2]\n",
    "        Z[:,3] = 2*x[:,0]\n",
    "        Z[:,4] = x[:,1]**2\n",
    "        Z[:,5] = 2*x[:,1]*x[:,2]\n",
    "        Z[:,6] = 2*x[:,1]\n",
    "        Z[:,7] = x[:,2]**2\n",
    "        Z[:,8] = 2*x[:,2]\n",
    "        Z[:,9] = 1\n",
    "        v, s, t = np.linalg.svd(Z,full_matrices=False)        \n",
    "        smallest_index = np.argmin(np.array(s))\n",
    "        T = np.array(t)\n",
    "        T = T[smallest_index,:]        \n",
    "        norm = np.linalg.norm(np.dot(Z,T), ord=2)**2\n",
    "        return norm\n",
    "\n",
    "    def _preprocess(self, hits):\n",
    "        \n",
    "        x = hits.x.values\n",
    "        y = hits.y.values\n",
    "        z = hits.z.values\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        hits['x2'] = x/r\n",
    "        hits['y2'] = y/r\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        hits['z2'] = z/r\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "        for i, rz_scale in enumerate(self.rz_scales):\n",
    "            X[:,i] = X[:,i] * rz_scale\n",
    "       \n",
    "        return X\n",
    "    def _init(self, dfh, w1, w2, w3, w4, w5, w6, w7, epsilon, Niter):\n",
    "        dfh['r'] = np.sqrt(dfh['x'].values ** 2 + dfh['y'].values ** 2 + dfh['z'].values ** 2)\n",
    "        dfh['rt'] = np.sqrt(dfh['x'].values ** 2 + dfh['y'].values ** 2)\n",
    "        dfh['a0'] = np.arctan2(dfh['y'].values, dfh['x'].values)\n",
    "        dfh['z1'] = dfh['z'].values / dfh['rt'].values\n",
    "        dfh['z2'] = dfh['z'].values / dfh['r'].values\n",
    "        dfh['s1'] = dfh['hit_id']\n",
    "        dfh['N1'] = 1\n",
    "        dfh['z1'] = dfh['z'].values / dfh['rt'].values\n",
    "        dfh['z2'] = dfh['z'].values / dfh['r'].values\n",
    "        dfh['x1'] = dfh['x'].values / dfh['y'].values\n",
    "        dfh['x2'] = dfh['x'].values / dfh['r'].values\n",
    "        dfh['x3'] = dfh['y'].values / dfh['r'].values\n",
    "        dfh['x4'] = dfh['rt'].values / dfh['r'].values\n",
    "        mm = 1\n",
    "        for ii in tqdm(range(int(Niter))):\n",
    "            mm = mm * (-1)\n",
    "            dfh['a1'] = dfh['a0'].values + mm * (dfh['rt'].values + 0.000005\n",
    "                                                 * dfh['rt'].values ** 2) / 1000 * (ii / 2) / 180 * np.pi\n",
    "            dfh['sina1'] = np.sin(dfh['a1'].values)\n",
    "            dfh['cosa1'] = np.cos(dfh['a1'].values)\n",
    "            ss = StandardScaler()\n",
    "            dfs = ss.fit_transform(dfh[['sina1', 'cosa1', 'z1', 'z2','x1','x2','x3','x4']].values)\n",
    "            cx = np.array([w1, w1, w2, w3, w4, w5, w6, w7])\n",
    "            dfs = np.multiply(dfs, cx)\n",
    "            clusters = DBSCAN(eps=epsilon, min_samples=1, metric=\"euclidean\", n_jobs=32).fit(dfs).labels_\n",
    "            dfh['s2'] = clusters\n",
    "            dfh['N2'] = dfh.groupby('s2')['s2'].transform('count')\n",
    "            maxs1 = dfh['s1'].max()\n",
    "            dfh.loc[(dfh['N2'] > dfh['N1']) & (dfh['N2'] < 20),'s1'] = dfh['s2'] + maxs1\n",
    "            dfh['N1'] = dfh.groupby('s1')['s1'].transform('count')\n",
    "        return dfh['s1'].values\n",
    "    def predict(self, hits):         \n",
    "        self.clusters = self._init(hits,2.7474448671796874,1.3649721713529086,0.7034918842926337,\n",
    "                                        0.0005549122352940002,0.023096034747190672,0.04619756315527515,\n",
    "                                        0.2437077420144654,0.009750302717746615,338)\n",
    "        X = self._preprocess(hits) \n",
    "        cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "                             metric='braycurtis',cluster_selection_method='leaf',algorithm='best', leaf_size=50)\n",
    "        labels = np.unique(self.clusters)\n",
    "        self._eliminate_outliers(labels,X)          \n",
    "        max_len = np.max(self.clusters)\n",
    "        mask = self.clusters == 0\n",
    "        self.clusters[mask] = cl.fit_predict(X[mask])+max_len\n",
    "        return self.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70612062632493a78bec5bd5c69c5d4d523b8b83",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Clusterer()\n",
    "labels = model.predict(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d87954eeb3413d98fab9732172c5ef56624602c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0bcf488d3b05ba63ad0b15b13db62b445ddbe3b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = create_one_event_submission(0, hits, labels)\n",
    "score = score_event(truth, submission)\n",
    "print(\"Your score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b20cf0c-2754-48dd-ab1e-4f489c2aa05c",
    "_uuid": "7f8de52b9022581bf10aa813d2db005b842f0be7",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_test = \"../input/test\"\n",
    "test_dataset_submissions = []\n",
    "\n",
    "create_submission = False # True for submission \n",
    "if create_submission:\n",
    "    for event_id, hits, cells in load_dataset(path_to_test, parts=['hits', 'cells']):\n",
    "\n",
    "        # Track pattern recognition \n",
    "        model = Clusterer()\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        test_dataset_submissions.append(one_submission)\n",
    "        \n",
    "        print('Event ID: ', event_id)\n",
    "\n",
    "    # Create submission file\n",
    "    submission = pd.concat(test_dataset_submissions, axis=0)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a333cd4-351e-4274-aa7c-4cf8ab7fca1a",
    "_uuid": "70ce31d93086e022159d6227f35c6488bf80eb22",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
